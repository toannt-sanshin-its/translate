# helper.py
import hashlib
import json
import os
from pathlib import Path
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer
from langchain.text_splitter import RecursiveCharacterTextSplitter

def fingerprint(text: str) -> str:
    """
    TÃ­nh MD5 fingerprint dÃ¹ng Ä‘á»ƒ deduplication.
    Input:
      - text: chuá»—i cáº§n fingerprint
    Tráº£ vá»:
      - fp: chuá»—i hex 32 kÃ½ tá»± (MD5)
    """
    return hashlib.md5(text.encode('utf-8')).hexdigest()

def should_add(text: str, seen_fps: set) -> bool:
    """
    Kiá»ƒm tra xem `text` Ä‘Ã£ tá»«ng add chÆ°a dá»±a trÃªn fingerprint:

    - Náº¿u chÆ°a: tÃ­nh fingerprint, add vÃ o `seen_fps`, tráº£ vá» True (Ä‘Æ°á»£c phÃ©p add).
    - Náº¿u Ä‘Ã£ cÃ³: tráº£ vá» False (skip).
    """
    fp = fingerprint(text)
    if fp in seen_fps:
        return False
    seen_fps.add(fp)
    return True

def load_jsonl(path):
    """
    Generator Ä‘á»c file .jsonl, má»—i dÃ²ng tráº£ vá» 1 dict.
    Usage:
      for obj in load_jsonl("data.jsonl"):
          ...
    """
    with open(path, encoding='utf-8') as f:
        for line in f:
            yield json.loads(line)

def ensure_dir(path):
    """
    Táº¡o thÆ° má»¥c náº¿u chÆ°a tá»“n táº¡i.
    Input:
      - path: pathlib.Path hoáº·c string
    """
    p = Path(path)
    p.mkdir(parents=True, exist_ok=True)
    return p

def make_embedding_tools(model_name: str, max_tokens: int, stride: int):
    """
    Tráº£ vá» tuple (emb_model, tokenizer, splitter) Ä‘Ã£ Ä‘Æ°á»£c cáº¥u hÃ¬nh sáºµn.

    - model_name: tÃªn hoáº·c Ä‘Æ°á»ng dáº«n cá»§a embedding model
    - max_tokens: kÃ­ch thÆ°á»›c tá»‘i Ä‘a má»—i chunk
    - stride: overlap giá»¯a cÃ¡c chunk
    """
    print("ðŸ”„ Náº¡p model embedding ...")
    # 1. Náº¡p model embedding
    emb_model = SentenceTransformer(model_name)

    # 2. Náº¡p tokenizer (dÃ¹ng Ä‘á»ƒ tÃ­nh Ä‘á»™ dÃ i token cho splitter)
    hf_tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True) # Chia chuá»—i text thÃ nh token IDs rá»“i ngÆ°á»£c láº¡i tá»« IDs thÃ nh text.

    # 3. Táº¡o RecursiveCharacterTextSplitter cho vÄƒn báº£n Nháº­t
    text_splitter = RecursiveCharacterTextSplitter(
        # cÃ¡c separator Æ°u tiÃªn cáº¯t: ngáº¯t cÃ¢u Nháº­t, dáº¥u xuá»‘ng dÃ²ng Ä‘Ã´i, rá»“i kÃ½ tá»± báº¥t ká»³
        separators=["ã€‚", "ï¼", "ï¼Ÿ", "\n\n", ""],
        chunk_size=max_tokens,
        chunk_overlap=stride,
        length_function=lambda txt: len(hf_tokenizer.encode(txt))
    )

    return emb_model, hf_tokenizer, text_splitter

def safe_get_str(meta: dict, *keys, default: str = "") -> str:
    """
    Summary: Truy xuáº¥t giÃ¡ trá»‹ lá»“ng nhau vÃ  tráº£ vá» dÆ°á»›i dáº¡ng chuá»—i.
    - Náº¿u lÃ  str thÃ¬ giá»¯ nguyÃªn.
    - Náº¿u lÃ  int/float/bool thÃ¬ chuyá»ƒn thÃ nh str.
    - NgÆ°á»£c láº¡i hoáº·c khÃ´ng tá»“n táº¡i thÃ¬ tráº£ default.
    """
    cur = meta
    for k in keys:
        if not isinstance(cur, dict) or k not in cur:
            return default
        cur = cur[k]
    if cur is None:
        return default
    if isinstance(cur, str):
        return cur
    if isinstance(cur, (int, float, bool)):
        return str(cur)
    return default

def safe_get_int(meta: dict, *keys, default: int = 0) -> int:
    """
    Summary: Truy xuáº¥t giÃ¡ trá»‹ lá»“ng nhau vÃ  Ã©p vá» int náº¿u Ä‘Æ°á»£c.
    - Chuyá»ƒn tá»« str náº¿u cÃ³ thá»ƒ.
    - Náº¿u lÃ  float hoáº·c bool thÃ¬ Ã©p vá» int.
    - Náº¿u khÃ´ng há»£p lá»‡ hoáº·c khÃ´ng tá»“n táº¡i thÃ¬ tráº£ default.
    """
    cur = meta
    for k in keys:
        if not isinstance(cur, dict) or k not in cur:
            return default
        cur = cur[k]
    try:
        if isinstance(cur, bool):  # trÃ¡nh bool bá»‹ hiá»ƒu lÃ  int khÃ´ng mong muá»‘n
            return int(cur)
        if isinstance(cur, (int, float)):
            return int(cur)
        if isinstance(cur, str):
            return int(cur.strip())
    except (ValueError, TypeError):
        pass
    return default

def safe_get_bool(meta: dict, *keys, default: bool = False) -> bool:
    """
    Summary: Truy xuáº¥t giÃ¡ trá»‹ lá»“ng nhau vÃ  quy vá» boolean.
    - Há»— trá»£ bool trá»±c tiáº¿p, sá»‘ (0=false, khÃ¡c=true), vÃ  chuá»—i nhÆ° 'true','1','yes','no'.
    - Náº¿u khÃ´ng xÃ¡c Ä‘á»‹nh Ä‘Æ°á»£c thÃ¬ tráº£ default.
    """
    cur = meta
    for k in keys:
        if not isinstance(cur, dict) or k not in cur:
            return default
        cur = cur[k]
    if isinstance(cur, bool):
        return cur
    if isinstance(cur, (int, float)):
        return bool(cur)
    if isinstance(cur, str):
        v = cur.strip().lower()
        if v in ("true", "1", "yes", "y", "t"):
            return True
        if v in ("false", "0", "no", "n", "f"):
            return False
    return default

def get_entry_type_by_text(metas, jp_text: str) -> str:
    """
    Summary: TÃ¬m entry cÃ³ text Ä‘Ãºng báº±ng jp_text, tráº£ vá» type top-level ('0' hoáº·c '1').
    Náº¿u khÃ´ng tÃ¬m tháº¥y hoáº·c type khÃ´ng há»£p lá»‡ thÃ¬ máº·c Ä‘á»‹nh '1'.
    """
    for m in metas:
        if safe_get_str(m, "text") == jp_text:
            t = safe_get_str(m, "type")  # top-level
            if t in ("0", "1"):
                return t
            break
    return "1"

# def evaluate_bleu(
#     emb,
#     index: faiss.Index,
#     metas: List[dict],
#     llm,
#     cfg,
#     test_data: List[Tuple[str, str]],
# ) -> float:
#     """
#     Compute corpus-level BLEU over a test set.
#     test_data: List of (jp_sentence, reference_translation).
#     Returns BLEU score in [0,100].
#     """
#     smoothie = SmoothingFunction().method1
#     references, hypotheses = [], []

#     for jp_sentence, gold_ref in test_data:
#         # Dá»‹ch cÃ¢u (Ä‘Ã£ bao gá»“m tÃ¡ch label segments)
#         pred = translate_one(emb, index, metas, llm, cfg, jp_sentence)

#         # Tokenize on whitespace
#         hyp_tokens = pred.strip().split()
#         ref_tokens = gold_ref.strip().split()

#         hypotheses.append(hyp_tokens)
#         # BLEU expects list of possible references per hypothesis
#         references.append([ref_tokens])

#     bleu = corpus_bleu(
#         references,
#         hypotheses,
#         weights=(0.25,0.25,0.25,0.25),
#         smoothing_function=smoothie
#     ) * 100.0

#     print(f"\n=== Corpus BLEU score: {bleu:.2f} ===")
#     return bleu