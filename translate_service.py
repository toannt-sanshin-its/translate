"""
FastAPI dá»‹ch Nháº­t -> Viá»‡t.
Æ¯u tiÃªn báº£n dá»‹ch Ä‘Ã£ lÆ°u trong FAISS (náº¿u gáº§n Ä‘Ãºng),
ngÆ°á»£c láº¡i fallback sang NLLB.

Cháº¡y:
    uvicorn translate_service:app --reload
"""

import pickle, faiss, numpy as np
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel

from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch

import config as C  # import cáº¥u hÃ¬nh á»Ÿ trÃªn

"""
pickle: Ä‘á»ƒ load file metadata pickle chá»©a thÃ´ng tin báº£n dá»‹ch.
faiss: thÆ° viá»‡n Facebook AI Similarity Search, dÃ¹ng Ä‘á»ƒ lÆ°u - truy váº¥n index embedding.
numpy: xá»­ lÃ½ máº£ng sá»‘ há»c, convert vector Ä‘á»ƒ FAISS search.
FastAPI, HTTPException:
    FastAPI: framework web dÃ¹ng Ä‘á»ƒ expose API.
    HTTPException: raise lá»—i tráº£ vá» cho client (vÃ­ dá»¥ khi input rá»—ng).
pydantic.BaseModel: DÃ¹ng define schema cho request/response, tá»± validate vÃ  tá»± sinh OpenAPI.

SentenceTransformer: Model embedding (vÃ­ dá»¥ all-MiniLMâ€¦) Ä‘á»ƒ chuyá»ƒn cÃ¢u Nháº­t thÃ nh vector.

AutoTokenizer, AutoModelForSeq2SeqLM: Tá»« HuggingFace Transformers, Ä‘á»ƒ load tokenizer + model NLLB-200.
torch: Ã¡nh xáº¡ model PyTorch lÃªn GPU/CPU.
import config as C: Táº¥t cáº£ tham sá»‘ cáº¥u hÃ¬nh (Ä‘Æ°á»ng dáº«n index, threshold, model names, thiáº¿t láº­p deviceâ€¦) gom chung trong config.py
"""

# ---------- Khá»Ÿi táº¡o ----------
app = FastAPI(title="JPâ†’VI Translator (FAISS + NLLB)")

print("ðŸš€ Loading embedding model ...")
emb_model = SentenceTransformer(C.EMB_MODEL, device=C.DEVICE)

print("ðŸš€ Loading FAISS index ...")
faiss_index = faiss.read_index(C.INDEX_PATH)
metas = pickle.load(open(C.META_PATH, "rb"))

print("ðŸš€ Loading NLLB model ...")
tok = AutoTokenizer.from_pretrained(C.NLLB_MODEL)
nllb = AutoModelForSeq2SeqLM.from_pretrained(C.NLLB_MODEL).to(C.DEVICE)

# ---------- Request / Response schemas ----------
class TranslationRequest(BaseModel):
    text: str

class TranslationResponse(BaseModel):
    translation: str
    source: str          # "database" hoáº·c "nllb"
    score: float | None  # cosine similarity náº¿u dÃ¹ng DB

# ---------- Helper ----------
def translate_with_nllb(jp_text: str) -> str:
    """
    Dá»‹ch báº±ng NLLBâ€‘200 (jpn_Jpan -> vie_Latn) :contentReference[oaicite:2]{index=2}
    """
    inputs = tok(
        jp_text,
        return_tensors="pt",
        src_lang=C.SRC_LANG
    ).to(C.DEVICE)

    output_tokens = nllb.generate(
        **inputs,
        forced_bos_token_id=tok.convert_tokens_to_ids(C.TGT_LANG),
        max_length=256
    )
    return tok.batch_decode(output_tokens, skip_special_tokens=True)[0]

# ---------- API ----------
@app.post("/translate", response_model=TranslationResponse)
def translate(req: TranslationRequest):
    if not req.text.strip():
        raise HTTPException(status_code=400, detail="Empty input")

    # 1) Embed & truy váº¥n FAISS
    vec = emb_model.encode(
        req.text,
        normalize_embeddings=True,
    ).astype("float32")
    D, I = faiss_index.search(np.expand_dims(vec, 0), C.TOP_K)  # D: cosine scores

    best_idx, best_score = int(I[0][0]), float(D[0][0])

    if best_score >= C.SIM_THRESHOLD:
        # CÃ³ báº£n dá»‹ch tin cáº­y trong DB
        meta = metas[best_idx]
        return TranslationResponse(
            translation=meta["metadata"]["translation"],
            source="database",
            score=best_score
        )

    # 2) Náº¿u khÃ´ng Ä‘á»§ tin cáº­y â†’ NLLB
    vi = translate_with_nllb(req.text)
    return TranslationResponse(
        translation=vi,
        source="nllb",
        score=best_score
    )
