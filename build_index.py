#!/usr/bin/env python
"""
Táº¡o FAISS index tá»« file .jsonl cÃ³ format:
{
  "id": "...",
  "text": "...",           # tiáº¿ng Nháº­t
  "metadata": {
      "language": "ja",
      "translation": "...",# tiáº¿ng Viá»‡t (náº¿u Ä‘Ã£ cÃ³)
      ...
  }
}

Cháº¡y:
    python build_index.py data/ja_vi.jsonl indexes/
"""

import argparse
import json
import pathlib
import pickle
import os
import numpy as np
import hashlib
import faiss
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer
from langchain.text_splitter import RecursiveCharacterTextSplitter

# --- 1. Äá»c tham sá»‘ CLI ---
def parse_args():
    parser = argparse.ArgumentParser(
        description="Build FAISS index cho dá»¯ liá»‡u Nháº­t-Viá»‡t (cÃ³ chunking)"
    )
    parser.add_argument("jsonl_path", help="ÄÆ°á»ng dáº«n file .jsonl")
    parser.add_argument("out_dir", help="ThÆ° má»¥c ghi index/metadata")
    parser.add_argument(
        "--max_tokens", type=int, default=256,
        help="Sá»‘ token tá»‘i Ä‘a má»—i chunk"
    )
    parser.add_argument(
        "--stride", type=int, default=50,
        help="Overlap token giá»¯a cÃ¡c chunk"
    )
    return parser.parse_args()

# --- 2. Hash fingerprint Ä‘á»ƒ dedup text ---
def fingerprint(text: str) -> str:
    return hashlib.md5(text.encode('utf-8')).hexdigest()

# --- 3. Load data vÃ  chunking ---
def load_and_chunk(jsonl_path, splitter):
    texts, metas = [], []
    for obj in load_jsonl(jsonl_path):
        if obj.get("metadata", {}).get("language") != "ja":
            continue
        chunks = splitter.split_text(obj["text"])
        for idx, txt in enumerate(chunks):
            texts.append(txt)
            metas.append({
                "id": f"{obj['id']}_chunk{idx}",
                "text": txt,
                "translation": obj["metadata"].get("translation", ""),
            })
    return texts, metas

def load_jsonl(path):
    with open(path, encoding="utf-8") as f:
        for line in f:
            yield json.loads(line)

def main():
    args = parse_args()

    # --- Thiáº¿t láº­p model & tokenizer ---
    print("ğŸ”„ Náº¡p model embedding ...")
    emb_model = SentenceTransformer(
        os.getenv("EMB_MODEL", "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
    )
    hf_tokenizer = AutoTokenizer.from_pretrained(
        os.getenv("EMB_MODEL", "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"),
        use_fast=True
    ) # Chia chuá»—i text thÃ nh token IDs rá»“i ngÆ°á»£c láº¡i tá»« IDs thÃ nh text.

    # Táº¡o Recursive splitter
    text_splitter = RecursiveCharacterTextSplitter(
        # cÃ¡c separator Æ°u tiÃªn cáº¯t: ngáº¯t cÃ¢u Nháº­t, dáº¥u xuá»‘ng dÃ²ng Ä‘Ã´i, rá»“i kÃ½ tá»± báº¥t ká»³
        separators=["ã€‚", "ï¼", "ï¼Ÿ", "\n\n", ""],
        chunk_size=args.max_tokens,
        chunk_overlap=args.stride,
        length_function=lambda x: len(hf_tokenizer.encode(x))
    )

    texts, metas = [], []
    print("ğŸ“¥ Äá»c & chunk dá»¯ liá»‡u... sá»­ dá»¥ng CharacterTextSplitter")
    for obj in load_jsonl(args.jsonl_path):
        if obj.get("metadata", {}).get("language") != "ja":
            continue
        chunks = text_splitter.split_text(obj["text"])
        for idx, txt in enumerate(chunks):
            texts.append(txt)
            metas.append({
                "id": f"{obj['id']}_chunk{idx}",
                "text": txt,
                # chá»‰ giá»¯ translation Ä‘á»ƒ tiáº¿t kiá»‡m metadata
                "translation": obj["metadata"].get("translation", ""),
            })

    # --- Encode vectors ---
    print("âš™ï¸ Encoding vectors...")
    vecs = emb_model.encode(
        texts,
        batch_size=64,
        show_progress_bar=True,
        normalize_embeddings=True          # L2â€‘norm â†’ cosine = innerâ€‘product
    ).astype("float32")

    # --- Build FAISS index ---
    dim = vecs.shape[1]    # chiá»u, Ä‘ang dÃ¹ng lÃ  384 chiá»u
    print('vecs', vecs)
    print('dim', dim)
    index = faiss.IndexFlatIP(dim)        # IP = cosine do vec Ä‘Ã£ chuáº©n hoÃ¡
    index.add(vecs)

    # --- Ghi ra Ä‘Ä©a ---
    out_dir = pathlib.Path(args.out_dir)
    out_dir.mkdir(parents=True, exist_ok=True)
    faiss.write_index(index, str(out_dir / "faiss.index"))
    with open(out_dir / "meta.pkl", "wb") as fp: # Má»Ÿ file á»Ÿ cháº¿ Ä‘á»™ nhá»‹ phÃ¢n (â€œwbâ€)
        pickle.dump(metas, fp) # biáº¿n thÃ nh 1 luá»“ng dá»¯ liá»‡u nhá»‹ phÃ¢n (gá»i lÃ  pickle stream) vÃ  ghi vÃ o file

    print(f"âœ… ÄÃ£ láº­p chá»‰ má»¥c {len(texts)} chunks â†’ {out_dir}")

if __name__ == "__main__":
    main()
