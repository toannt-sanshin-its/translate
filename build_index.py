#!/usr/bin/env python
"""
T·∫°o FAISS index t·ª´ file .jsonl c√≥ format:
{
  "id": "...",
  "text": "...",           # ti·∫øng Nh·∫≠t
  "metadata": {
      "language": "ja",
      "translation": "...",# ti·∫øng Vi·ªát (n·∫øu ƒë√£ c√≥)
      ...
  }
}

Ch·∫°y:
    python build_index.py data/ja_vi.jsonl indexes/
"""

import argparse, json, pathlib, pickle, os
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer
from langchain.text_splitter import CharacterTextSplitter

# --- 1. ƒê·ªçc tham s·ªë CLI ---
def parse_args():
    parser = argparse.ArgumentParser(
        description="Build FAISS index cho d·ªØ li·ªáu Nh·∫≠t-Vi·ªát (c√≥ chunking)"
    )
    parser.add_argument("jsonl_path", help="ƒê∆∞·ªùng d·∫´n file .jsonl")
    parser.add_argument("out_dir", help="Th∆∞ m·ª•c ghi index/metadata")
    parser.add_argument(
        "--max_tokens", type=int, default=256,
        help="S·ªë token t·ªëi ƒëa m·ªói chunk"
    )
    parser.add_argument(
        "--stride", type=int, default=50,
        help="Overlap token gi·ªØa c√°c chunk"
    )
    return parser.parse_args()

def load_jsonl(path):
    with open(path, encoding="utf-8") as f:
        for line in f:
            yield json.loads(line)

def chunk_text(text: str, tokenizer, max_len: int, stride: int):
    ids = tokenizer.encode(text)
    chunks = []
    start = 0
    while start < len(ids):
        end = min(start + max_len, len(ids))
        chunk_ids = ids[start:end]
        chunk_text = tokenizer.decode(chunk_ids, skip_special_tokens=True)
        chunks.append((start, end, chunk_text))
        if end == len(ids):
            break
        start += max_len - stride
    return chunks

def main():
    args = parse_args()

    # --- Thi·∫øt l·∫≠p model & tokenizer ---
    print("üîÑ N·∫°p model embedding ...")
    emb_model = SentenceTransformer(
        os.getenv("EMB_MODEL", "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
    )
    hf_tokenizer = AutoTokenizer.from_pretrained(
        os.getenv("EMB_MODEL", "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"),
        use_fast=True
    ) # Chia chu·ªói text th√†nh token IDs r·ªìi ng∆∞·ª£c l·∫°i t·ª´ IDs th√†nh text.

    # --- Thi·∫øt l·∫≠p Text Splitter ---
    text_splitter = CharacterTextSplitter(
        chunk_size=args.max_tokens,
        chunk_overlap=args.stride,
        length_function=lambda x: len(hf_tokenizer.encode(x))
    )

    texts, metas = [], []
    print("üì• ƒê·ªçc & chunk d·ªØ li·ªáu... s·ª≠ d·ª•ng CharacterTextSplitter")
    for obj in load_jsonl(args.jsonl_path):
        if obj.get("metadata", {}).get("language") != "ja":
            continue
        chunks = text_splitter.split_text(obj["text"])
        for idx, txt in enumerate(chunks):
            texts.append(txt)
            metas.append({
                "id": f"{obj['id']}_chunk{idx}",
                "text": txt,
                # ch·ªâ gi·ªØ translation ƒë·ªÉ ti·∫øt ki·ªám metadata
                "translation": obj["metadata"].get("translation", ""),
            })

    # --- Encode vectors ---
    print("‚öôÔ∏è Encoding vectors...")
    vecs = emb_model.encode(
        texts,
        batch_size=64,
        show_progress_bar=True,
        normalize_embeddings=True          # L2‚Äënorm ‚Üí cosine = inner‚Äëproduct
    ).astype("float32")

    # --- Build FAISS index ---
    dim = vecs.shape[1]    # chi·ªÅu, ƒëang d√πng l√† 384 chi·ªÅu
    print('vecs', vecs)
    print('dim', dim)
    index = faiss.IndexFlatIP(dim)        # IP = cosine do vec ƒë√£ chu·∫©n ho√°
    index.add(vecs)

    # --- Ghi ra ƒëƒ©a ---
    out_dir = pathlib.Path(args.out_dir)
    out_dir.mkdir(parents=True, exist_ok=True)
    faiss.write_index(index, str(out_dir / "faiss.index"))
    with open(out_dir / "meta.pkl", "wb") as fp: # M·ªü file ·ªü ch·∫ø ƒë·ªô nh·ªã ph√¢n (‚Äúwb‚Äù)
        pickle.dump(metas, fp) # bi·∫øn th√†nh 1 lu·ªìng d·ªØ li·ªáu nh·ªã ph√¢n (g·ªçi l√† pickle stream) v√† ghi v√†o file

    print(f"‚úÖ ƒê√£ l·∫≠p ch·ªâ m·ª•c {len(texts)} chunks ‚Üí {out_dir}")

if __name__ == "__main__":
    main()
