#!/usr/bin/env python
"""
Ch·∫°y:
    Ch·∫°y l·∫ßn ƒë·∫ßu init vectordb: python build_index.py data/ja_vi.jsonl indexes/
    Ch·∫°y ƒë·ªÉ th√™m data v√†o vectordb python build_index.py --extend data/new_chunks.jsonl indexes/
"""

import argparse
import pathlib
import pickle
import os
import faiss
from helper import fingerprint, load_jsonl, should_add, make_embedding_tools
import re

# --- CLI ---
def parse_args():
    parser = argparse.ArgumentParser(
        description="Build FAISS index cho d·ªØ li·ªáu Nh·∫≠t-Vi·ªát (c√≥ chunking)"
    )
    parser.add_argument("jsonl_path", help="Input JSONL file")
    parser.add_argument("out_dir", help="Th∆∞ m·ª•c ghi index/metadata")
    parser.add_argument("--max_tokens", type=int, default=256, help="S·ªë token t·ªëi ƒëa m·ªói chunk")
    parser.add_argument("--stride", type=int, default=50, help="Overlap token gi·ªØa c√°c chunk")
    parser.add_argument("--extend", action='store_true', 
                        help="Extend existing index (with dedup)") # N·∫øu c√≥ --extend ‚Üí g√°n args.extend = True; n·∫øu kh√¥ng c√≥ ‚Üí False
    return parser.parse_args()

# --- Load data v√† chunking ---
def load_and_chunk(jsonl_path, splitter, seen_fps=None, start_doc: int = 1):
    texts, metas = [], []
    dedup = seen_fps is not None
    current_doc = start_doc

    for obj in load_jsonl(jsonl_path):
        if obj.get("metadata", {}).get("language") != "ja":
            continue
        chunks = splitter.split_text(obj["text"])
        for idx, txt in enumerate(chunks):
            if dedup:
                if not should_add(txt, seen_fps): # n·∫øu c√≥ trong fingerprinter th√¨ ko cho add v√†o db
                    continue
            texts.append(txt)
            metas.append({
                # "id": f"doc{obj['id']}_chunk{idx}",
                "id": f"{current_doc}_{idx}",
                "text": txt,
                "translation": obj["metadata"].get("translation", ""),
                "type": obj["metadata"].get("type", ""),
            })
        current_doc += 1

    return texts, metas  

def parse_doc_number_from_id(id_str: str) -> int:
    """
    Summary: L·∫•y s·ªë doc t·ª´ id ki·ªÉu '1235_1'. Tr·∫£ v·ªÅ 0 n·∫øu kh√¥ng parse ƒë∆∞·ª£c.
    """
    # H·ªó tr·ª£ c·∫£ hai format: '1235_1'
    m = re.match(r'^(\d+)_', id_str)
    if m:
        return int(m.group(1))
    return 0

def max_existing_doc_number(metas) -> int:
    """
    Summary: Duy·ªát metas, t√¨m doc number l·ªõn nh·∫•t t·ª´ id v√† tr·∫£ l·∫°i.
    """
    max_n = 0
    for m in metas:
        id_str = m.get("id", "")
        n = parse_doc_number_from_id(id_str)
        if n > max_n:
            max_n = n
    return max_n

# --- 1. Build m·ªõi (no dedup) ---
def init_index(jsonl_path: str, index_dir: pathlib.Path, max_tokens: int, stride: int):
    # Paths
    idx_path = index_dir / 'faiss.index'
    meta_path = index_dir / 'meta.pkl'
    fps_path = index_dir / 'seen_fps.pkl'

    # T√≠nh doc s·ªë b·∫Øt ƒë·∫ßu: l·∫•y max hi·ªán c√≥ r·ªìi +1
    start_doc = 1

    # --- Thi·∫øt l·∫≠p model & tokenizer ---
    model_name = os.getenv("EMB_MODEL", "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
    emb_model, hf_tokenizer, text_splitter = make_embedding_tools(
        model_name,
        max_tokens=max_tokens,
        stride=stride
    )

    # Read & chunk
    print("üì• ƒê·ªçc & chunk d·ªØ li·ªáu... s·ª≠ d·ª•ng RecursiveCharacterTextSplitter")
    texts, metas = load_and_chunk(jsonl_path, text_splitter, seen_fps=None, start_doc=start_doc)
    # --- Encode vectors ---
    print("‚öôÔ∏è Encoding vectors...")
    vecs = emb_model.encode(
        texts,
        batch_size=64,
        show_progress_bar=True,
        normalize_embeddings=True          # L2‚Äënorm ‚Üí cosine = inner‚Äëproduct
    ).astype("float32")

    # --- Build FAISS index ---
    dim = vecs.shape[1]    # chi·ªÅu, ƒëang d√πng l√† 384 chi·ªÅu
    index = faiss.IndexFlatIP(dim)        # IP = cosine do vec ƒë√£ chu·∫©n ho√°
    index.add(vecs)

    # Prepare seen_fps (all new)
    seen_fps = {fingerprint(txt) for txt in texts}

    # Persist
    index_dir.mkdir(parents=True, exist_ok=True)
    faiss.write_index(index, str(idx_path))
    pickle.dump(metas, meta_path.open('wb'))
    pickle.dump(seen_fps, fps_path.open('wb'))

    print(f"‚úÖ Initialized index: {len(texts)} chunks added.")

# --- 2. Extend (with dedup) ---
def extend_index(jsonl_path: str, index_dir: pathlib.Path, max_tokens: int, stride: int):
    # Paths
    idx_path = index_dir / 'faiss.index'
    meta_path = index_dir / 'meta.pkl'
    fps_path = index_dir / 'seen_fps.pkl'

    # Load previous state
    index = faiss.read_index(str(idx_path))
    metas = pickle.load(meta_path.open('rb'))
    seen_fps = pickle.load(fps_path.open('rb'))

    # T√≠nh doc s·ªë b·∫Øt ƒë·∫ßu: l·∫•y max hi·ªán c√≥ r·ªìi +1
    current_max_doc = max_existing_doc_number(metas)
    start_doc = current_max_doc + 1

    # --- Thi·∫øt l·∫≠p model & tokenizer ---
    model_name = os.getenv("EMB_MODEL", "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
    emb_model, hf_tokenizer, text_splitter = make_embedding_tools(
        model_name,
        max_tokens=max_tokens,
        stride=stride
    )

    # Process new file
    print("üì• ƒê·ªçc & chunk d·ªØ li·ªáu... s·ª≠ d·ª•ng RecursiveCharacterTextSplitter")
    new_texts, new_metas = load_and_chunk(jsonl_path, text_splitter, seen_fps, start_doc=start_doc)
    # Batch encode & add
    new_count = len(new_texts)
    if new_count:
        vecs = emb_model.encode(new_texts, normalize_embeddings=True).astype('float32')
        index.add(vecs)
        metas.extend(new_metas)

    # Persist updated state
    faiss.write_index(index, str(idx_path))
    pickle.dump(metas, meta_path.open('wb'))
    pickle.dump(seen_fps, fps_path.open('wb'))

    print(f"‚úÖ Extended index: {new_count} new chunks added."
          f" Total now: {index.ntotal}")

def main():
    args = parse_args()
    idx_dir = pathlib.Path(args.out_dir)

    if args.extend and idx_dir.joinpath('faiss.index').exists():
        extend_index(
            jsonl_path=args.jsonl_path,
            index_dir=idx_dir,
            max_tokens=args.max_tokens,
            stride=args.stride
        )
    else:
        init_index(
            jsonl_path=args.jsonl_path,
            index_dir=idx_dir,
            max_tokens=args.max_tokens,
            stride=args.stride
        )

# ----------------------- MAIN ------------------------        
if __name__ == "__main__":
    main()
